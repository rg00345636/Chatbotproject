{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7378154,"sourceType":"datasetVersion","datasetId":4287590}],"dockerImageVersionId":30636,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# RAG Chatbot with PineCone VectorDB","metadata":{}},{"cell_type":"code","source":"!pip install -qU \\\n    langchain==0.0.292 \\\n    openai==0.28.0 \\\n    datasets==2.14.5 \\\n    pinecone-client==2.2.4 \\\n    tiktoken==0.5.1 \\\n    cohere==4.27","metadata":{"execution":{"iopub.status.busy":"2024-01-11T16:29:18.033134Z","iopub.execute_input":"2024-01-11T16:29:18.033488Z","iopub.status.idle":"2024-01-11T16:29:36.642523Z","shell.execute_reply.started":"2024-01-11T16:29:18.033461Z","shell.execute_reply":"2024-01-11T16:29:36.641375Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Building a Chatbot (no RAG)","metadata":{}},{"cell_type":"markdown","source":"We will be relying heavily on the LangChain library to bring together the different components needed for our chatbot. To begin, we'll create a simple chatbot without any retrieval augmentation. We do this by initializing a `ChatOpenAI` object. For this we do need an [OpenAI API key](https://platform.openai.com/account/api-keys).","metadata":{}},{"cell_type":"code","source":"import os\nfrom langchain.chat_models import ChatOpenAI\n\nos.environ[\"OPENAI_API_KEY\"] = \"MY OPENAI KEY\"  # Changed due to security reasons\n\nchat = ChatOpenAI(\n    openai_api_key=os.environ[\"OPENAI_API_KEY\"],\n    model='gpt-3.5-turbo'\n)","metadata":{"execution":{"iopub.status.busy":"2024-01-11T16:30:13.143618Z","iopub.execute_input":"2024-01-11T16:30:13.144122Z","iopub.status.idle":"2024-01-11T16:30:13.149350Z","shell.execute_reply.started":"2024-01-11T16:30:13.144094Z","shell.execute_reply":"2024-01-11T16:30:13.148402Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Chats with OpenAI's `gpt-3.5-turbo` and `gpt-4` chat models are typically structured (in plain text) like this:\n\n```\nSystem: You are a helpful assistant.\n\nUser: Hi AI, how are you today?\n\nAssistant: I'm great thank you. How can I help you?\n\nUser: I'd like to understand string theory.\n\nAssistant:\n```\n\nThe final `\"Assistant:\"` without a response is what would prompt the model to continue the conversation. In the official OpenAI `ChatCompletion` endpoint these would be passed to the model in a format like:\n\n```python\n[\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n    {\"role\": \"user\", \"content\": \"Hi AI, how are you today?\"},\n    {\"role\": \"assistant\", \"content\": \"I'm great thank you. How can I help you?\"}\n    {\"role\": \"user\", \"content\": \"I'd like to understand string theory.\"}\n]\n```\n\n","metadata":{}},{"cell_type":"code","source":"from langchain.schema import (\n    SystemMessage,\n    HumanMessage,\n    AIMessage\n)\n\nmessages = [\n    SystemMessage(content=\"You are a helpful assistant.\"),\n    HumanMessage(content=\"Hi AI, how are you today?\"),\n    AIMessage(content=\"I'm great thank you. How can I help you?\"),\n    HumanMessage(content=\"I'd like to understand string theory.\")\n]","metadata":{"execution":{"iopub.status.busy":"2024-01-11T16:30:42.966499Z","iopub.execute_input":"2024-01-11T16:30:42.966880Z","iopub.status.idle":"2024-01-11T16:30:42.972310Z","shell.execute_reply.started":"2024-01-11T16:30:42.966851Z","shell.execute_reply":"2024-01-11T16:30:42.971234Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Swap the role of `\"user\"` for `HumanMessage`, and the role of `\"assistant\"` for `AIMessage`.\n\nResponse from Chat Open AI - ----------","metadata":{}},{"cell_type":"code","source":"res = chat(messages)\nres","metadata":{"execution":{"iopub.status.busy":"2024-01-11T16:30:46.874958Z","iopub.execute_input":"2024-01-11T16:30:46.875553Z","iopub.status.idle":"2024-01-11T16:31:02.123855Z","shell.execute_reply.started":"2024-01-11T16:30:46.875519Z","shell.execute_reply":"2024-01-11T16:31:02.122914Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In response we get another AI message object. We can print it more clearly like so:","metadata":{}},{"cell_type":"code","source":"print(res.content)","metadata":{"execution":{"iopub.status.busy":"2024-01-11T16:31:02.125666Z","iopub.execute_input":"2024-01-11T16:31:02.126067Z","iopub.status.idle":"2024-01-11T16:31:02.131518Z","shell.execute_reply.started":"2024-01-11T16:31:02.126033Z","shell.execute_reply":"2024-01-11T16:31:02.130574Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Add another message and generate the response","metadata":{}},{"cell_type":"code","source":"# add latest AI response to messages\nmessages.append(res)\n\n# now create a new user prompt\nprompt = HumanMessage(\n    content=\"Why do physicists believe it can produce a 'unified theory'?\"\n)\n# add to messages\nmessages.append(prompt)\n\n# send to chat-gpt\nres = chat(messages)\n\nprint(res.content)","metadata":{"execution":{"iopub.status.busy":"2024-01-11T16:31:02.132682Z","iopub.execute_input":"2024-01-11T16:31:02.132998Z","iopub.status.idle":"2024-01-11T16:31:19.704056Z","shell.execute_reply.started":"2024-01-11T16:31:02.132967Z","shell.execute_reply":"2024-01-11T16:31:19.703176Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Importing the Data","metadata":{}},{"cell_type":"markdown","source":"Dataset -  Hugging Face Datasets library to load our data. Dataset - scientific_papers , which will serve as the external knowledge base for the chatbot.","metadata":{}},{"cell_type":"markdown","source":"#Login to HuggingFace","metadata":{}},{"cell_type":"code","source":"from huggingface_hub import notebook_login\nnotebook_login()","metadata":{"execution":{"iopub.status.busy":"2024-01-11T07:10:22.085012Z","iopub.execute_input":"2024-01-11T07:10:22.085392Z","iopub.status.idle":"2024-01-11T07:10:22.305522Z","shell.execute_reply.started":"2024-01-11T07:10:22.085361Z","shell.execute_reply":"2024-01-11T07:10:22.304680Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install -U datasets","metadata":{"execution":{"iopub.status.busy":"2024-01-11T16:31:28.210287Z","iopub.execute_input":"2024-01-11T16:31:28.210964Z","iopub.status.idle":"2024-01-11T16:31:41.265193Z","shell.execute_reply.started":"2024-01-11T16:31:28.210930Z","shell.execute_reply":"2024-01-11T16:31:41.264105Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from datasets import load_dataset","metadata":{"execution":{"iopub.status.busy":"2024-01-11T16:31:41.267180Z","iopub.execute_input":"2024-01-11T16:31:41.267492Z","iopub.status.idle":"2024-01-11T16:31:42.336510Z","shell.execute_reply.started":"2024-01-11T16:31:41.267463Z","shell.execute_reply":"2024-01-11T16:31:42.335736Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from datasets import load_dataset\ndataset = load_dataset(\"scientific_papers\",\"arxiv\")\n#dataset = load_dataset(\"arxiv_dataset\",\"arxiv\")\n\ndataset","metadata":{"execution":{"iopub.status.busy":"2024-01-11T16:31:47.942042Z","iopub.execute_input":"2024-01-11T16:31:47.943291Z","iopub.status.idle":"2024-01-11T16:37:39.300244Z","shell.execute_reply.started":"2024-01-11T16:31:47.943246Z","shell.execute_reply":"2024-01-11T16:37:39.299354Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n!pip install -qU \\\n  transformers==4.33.1 \\\n  sentence-transformers==2.2.2 \\\n  pinecone-client==2.2.2 \\\n  datasets==2.14.0 \\\n  accelerate==0.21.0 \\\n  einops==0.6.1 \\\n  langchain==0.0.240 \\\n  xformers==0.0.20 \\\n  bitsandbytes==0.41.0","metadata":{"execution":{"iopub.status.busy":"2024-01-11T16:37:39.301867Z","iopub.execute_input":"2024-01-11T16:37:39.302145Z","iopub.status.idle":"2024-01-11T16:40:43.243069Z","shell.execute_reply.started":"2024-01-11T16:37:39.302121Z","shell.execute_reply":"2024-01-11T16:40:43.241777Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch import cuda\nfrom langchain.embeddings.huggingface import HuggingFaceEmbeddings\n\nembed_model_id = 'sentence-transformers/all-MiniLM-L6-v2'\n\ndevice = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'\n\nembed_model = HuggingFaceEmbeddings(\n    model_name=embed_model_id,\n    model_kwargs={'device': device},\n    encode_kwargs={'device': device, 'batch_size': 32}\n)","metadata":{"execution":{"iopub.status.busy":"2024-01-11T16:41:40.198443Z","iopub.execute_input":"2024-01-11T16:41:40.198857Z","iopub.status.idle":"2024-01-11T16:41:59.825138Z","shell.execute_reply.started":"2024-01-11T16:41:40.198826Z","shell.execute_reply":"2024-01-11T16:41:59.824331Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ndocs = [\n    \"this is one document\",\n    \"and another document\"\n]\n\nembeddings = embed_model.embed_documents(docs)\n\nprint(f\"We have {len(embeddings)} doc embeddings, each with \"\n      f\"a dimensionality of {len(embeddings[0])}.\")","metadata":{"execution":{"iopub.status.busy":"2024-01-11T16:41:59.826576Z","iopub.execute_input":"2024-01-11T16:41:59.826871Z","iopub.status.idle":"2024-01-11T16:42:02.678289Z","shell.execute_reply.started":"2024-01-11T16:41:59.826846Z","shell.execute_reply":"2024-01-11T16:42:02.677175Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Use PineCone DB as Vector DB ","metadata":{}},{"cell_type":"code","source":"import os\nimport pinecone\n\n# get API key from app.pinecone.io and environment from console\npinecone.init(\n    api_key='My Pinecone Key', # Changed due to security reasons\n    environment='gcp-starter'\n)","metadata":{"execution":{"iopub.status.busy":"2024-01-11T16:42:05.993631Z","iopub.execute_input":"2024-01-11T16:42:05.994457Z","iopub.status.idle":"2024-01-11T16:42:06.804122Z","shell.execute_reply.started":"2024-01-11T16:42:05.994424Z","shell.execute_reply":"2024-01-11T16:42:06.803197Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import time\n\nindex_name = 'llama-2-rag'\n\nif index_name not in pinecone.list_indexes():\n    pinecone.create_index(\n        index_name,\n        dimension=len(embeddings[0]),\n        metric='cosine'\n    )\n    # wait for index to finish initialization\n    while not pinecone.describe_index(index_name).status['ready']:\n        time.sleep(1)","metadata":{"execution":{"iopub.status.busy":"2024-01-11T16:42:39.159493Z","iopub.execute_input":"2024-01-11T16:42:39.160441Z","iopub.status.idle":"2024-01-11T16:42:39.258074Z","shell.execute_reply.started":"2024-01-11T16:42:39.160405Z","shell.execute_reply":"2024-01-11T16:42:39.257357Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nindex = pinecone.Index(index_name)\nindex.describe_index_stats()","metadata":{"execution":{"iopub.status.busy":"2024-01-11T16:42:48.343473Z","iopub.execute_input":"2024-01-11T16:42:48.343852Z","iopub.status.idle":"2024-01-11T16:42:48.498664Z","shell.execute_reply.started":"2024-01-11T16:42:48.343823Z","shell.execute_reply":"2024-01-11T16:42:48.497793Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install datasets==2.15.0","metadata":{"execution":{"iopub.status.busy":"2024-01-11T16:43:12.175411Z","iopub.execute_input":"2024-01-11T16:43:12.175762Z","iopub.status.idle":"2024-01-11T16:43:25.275290Z","shell.execute_reply.started":"2024-01-11T16:43:12.175737Z","shell.execute_reply":"2024-01-11T16:43:25.274089Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#!pip install -U transformers --no-index --find-links=file:///kaggle/input/huggingfaces/transformers","metadata":{"execution":{"iopub.status.busy":"2024-01-10T19:25:29.415670Z","iopub.execute_input":"2024-01-10T19:25:29.415997Z","iopub.status.idle":"2024-01-10T19:25:41.402528Z","shell.execute_reply.started":"2024-01-10T19:25:29.415970Z","shell.execute_reply":"2024-01-10T19:25:41.401271Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\n\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"","metadata":{"execution":{"iopub.status.busy":"2024-01-10T19:25:41.404948Z","iopub.execute_input":"2024-01-10T19:25:41.405356Z","iopub.status.idle":"2024-01-10T19:25:41.410613Z","shell.execute_reply.started":"2024-01-10T19:25:41.405320Z","shell.execute_reply":"2024-01-10T19:25:41.409790Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### List Huggingface datasets to verify","metadata":{}},{"cell_type":"code","source":"from huggingface_hub import list_datasets\nprint([dataset.id for dataset in list_datasets()])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torchtext\nprint(torchtext.__version__)\n","metadata":{"execution":{"iopub.status.busy":"2024-01-11T07:47:12.602698Z","iopub.execute_input":"2024-01-11T07:47:12.603509Z","iopub.status.idle":"2024-01-11T07:47:12.608410Z","shell.execute_reply.started":"2024-01-11T07:47:12.603475Z","shell.execute_reply":"2024-01-11T07:47:12.607494Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install torchtext==0.16.2","metadata":{"execution":{"iopub.status.busy":"2024-01-11T07:44:36.289782Z","iopub.execute_input":"2024-01-11T07:44:36.290416Z","iopub.status.idle":"2024-01-11T07:46:42.854393Z","shell.execute_reply.started":"2024-01-11T07:44:36.290382Z","shell.execute_reply":"2024-01-11T07:46:42.853049Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Dataset \n\n**L**arge **L**anguage **M**odels (LLMs) cannot answer the queries specific to our dataset - scientific papers from HuggingFace","metadata":{}},{"cell_type":"markdown","source":"### Task 4: Building the Knowledge Base","metadata":{}},{"cell_type":"markdown","source":"Vector DB - Pinecone \n\nSetup","metadata":{}},{"cell_type":"code","source":"import pinecone\n\n# get API key from app.pinecone.io and environment from console\npinecone.init(\n    api_key=\"My PineCone Key\", # Changed due to security reasons\n    environment=\"gcp-starter\"\n)","metadata":{"execution":{"iopub.status.busy":"2024-01-11T16:44:10.348277Z","iopub.execute_input":"2024-01-11T16:44:10.349208Z","iopub.status.idle":"2024-01-11T16:44:10.437615Z","shell.execute_reply.started":"2024-01-11T16:44:10.349174Z","shell.execute_reply":"2024-01-11T16:44:10.436705Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Then we initialize the index.Use OpenAI's `text-embedding-ada-002` model for creating the embeddings, so we set the `dimension` to `1536`.","metadata":{}},{"cell_type":"code","source":"import time\n\nindex_name = 'llama-2-rag'\npinecone.delete_index(index_name)\nif index_name not in pinecone.list_indexes():\n    pinecone.create_index(\n        index_name,\n        dimension=1536,\n        metric='cosine'\n    )\n    # wait for index to finish initialization\n    while not pinecone.describe_index(index_name).status['ready']:\n        time.sleep(1)\n\nindex = pinecone.Index(index_name)","metadata":{"execution":{"iopub.status.busy":"2024-01-11T16:44:16.748339Z","iopub.execute_input":"2024-01-11T16:44:16.748725Z","iopub.status.idle":"2024-01-11T16:44:27.866615Z","shell.execute_reply.started":"2024-01-11T16:44:16.748694Z","shell.execute_reply":"2024-01-11T16:44:27.865570Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Use the index","metadata":{}},{"cell_type":"code","source":"index.describe_index_stats()","metadata":{"execution":{"iopub.status.busy":"2024-01-11T16:44:37.071227Z","iopub.execute_input":"2024-01-11T16:44:37.071964Z","iopub.status.idle":"2024-01-11T16:44:37.236880Z","shell.execute_reply.started":"2024-01-11T16:44:37.071928Z","shell.execute_reply":"2024-01-11T16:44:37.236016Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Use the model from OpenAI using Langchain","metadata":{}},{"cell_type":"code","source":"from langchain.embeddings.openai import OpenAIEmbeddings\n\nembed_model = OpenAIEmbeddings(model=\"text-embedding-ada-002\")","metadata":{"execution":{"iopub.status.busy":"2024-01-11T16:44:45.954139Z","iopub.execute_input":"2024-01-11T16:44:45.954776Z","iopub.status.idle":"2024-01-11T16:44:45.959597Z","shell.execute_reply.started":"2024-01-11T16:44:45.954740Z","shell.execute_reply":"2024-01-11T16:44:45.958570Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Embedding creation","metadata":{}},{"cell_type":"code","source":"! pip install tiktoken ","metadata":{"execution":{"iopub.status.busy":"2024-01-11T16:44:58.242423Z","iopub.execute_input":"2024-01-11T16:44:58.242785Z","iopub.status.idle":"2024-01-11T16:45:10.506093Z","shell.execute_reply.started":"2024-01-11T16:44:58.242760Z","shell.execute_reply":"2024-01-11T16:45:10.504814Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"texts = [\n    'this is the first chunk of text',\n    'then another second chunk of text is here'\n]\n\nres = embed_model.embed_documents(texts)\nlen(res), len(res[0])","metadata":{"execution":{"iopub.status.busy":"2024-01-11T16:45:10.508131Z","iopub.execute_input":"2024-01-11T16:45:10.508454Z","iopub.status.idle":"2024-01-11T16:45:12.046246Z","shell.execute_reply.started":"2024-01-11T16:45:10.508425Z","shell.execute_reply":"2024-01-11T16:45:12.045307Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From this we get two (aligning to our two chunks of text) 1536-dimensional embeddings.\n\nWe're now ready to embed and index all our our data! We do this by looping through our dataset and embedding and inserting everything in batches.","metadata":{}},{"cell_type":"code","source":"import uuid\nfrom tqdm import tqdm\n\ndocuments = dataset[\"train\"][\"abstract\"]\nbatch_size = 20\nfor i in tqdm(range(0, len(documents), batch_size)):\n    i_end = min(len(documents), i+batch_size)\n    batch = documents[i:i_end]\n    embeddings = embed_model.embed_documents(batch)\n    ids = [uuid.uuid4().hex for _ in batch]\n    metadata = [\n        {'text': x} for x in batch\n    ]\n    print(i_end)\n    index.upsert(vectors=zip(ids, embeddings, metadata))","metadata":{"execution":{"iopub.status.busy":"2024-01-11T16:45:59.259771Z","iopub.execute_input":"2024-01-11T16:45:59.260779Z","iopub.status.idle":"2024-01-11T16:47:43.818867Z","shell.execute_reply.started":"2024-01-11T16:45:59.260744Z","shell.execute_reply":"2024-01-11T16:47:43.817195Z"},"_kg_hide-output":true,"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Even though we have error , small amount of data is loaded for our current chatbot ,  vector index has been populated - `describe_index_stats` ","metadata":{}},{"cell_type":"code","source":"index.describe_index_stats()","metadata":{"execution":{"iopub.status.busy":"2024-01-11T16:49:12.174174Z","iopub.execute_input":"2024-01-11T16:49:12.175094Z","iopub.status.idle":"2024-01-11T16:49:12.241640Z","shell.execute_reply.started":"2024-01-11T16:49:12.175053Z","shell.execute_reply":"2024-01-11T16:49:12.240703Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### RAG Chatbot - Retrieval Augmented Generation","metadata":{}},{"cell_type":"markdown","source":"load the LangChain abstraction for a vector index, called a `vectorstore`","metadata":{}},{"cell_type":"code","source":"from langchain.vectorstores import Pinecone\n\ntext_field = \"text\"  # the metadata field that contains our text\n\n# initialize the vector store object\nvectorstore = Pinecone(\n    index, embed_model.embed_query, text_field)","metadata":{"execution":{"iopub.status.busy":"2024-01-11T16:49:19.260970Z","iopub.execute_input":"2024-01-11T16:49:19.261359Z","iopub.status.idle":"2024-01-11T16:49:19.268063Z","shell.execute_reply.started":"2024-01-11T16:49:19.261330Z","shell.execute_reply":"2024-01-11T16:49:19.267212Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Query the index and see if we have any relevant information for the question from the scientific paper","metadata":{}},{"cell_type":"code","source":"query = \"What is leptonic delay?\"\n\nvectorstore.similarity_search(query, k=3)","metadata":{"execution":{"iopub.status.busy":"2024-01-11T16:49:26.935207Z","iopub.execute_input":"2024-01-11T16:49:26.935692Z","iopub.status.idle":"2024-01-11T16:49:27.429120Z","shell.execute_reply.started":"2024-01-11T16:49:26.935648Z","shell.execute_reply":"2024-01-11T16:49:27.428174Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This information is not clear . Our LLM will be able to parse this information much faster than us. Lets  connect the output from our `vectorstore` to our `chat` chatbot. To do that we can use the same logic as we used earlier.","metadata":{}},{"cell_type":"code","source":"def augment_prompt(query: str):\n    # get top 3 results from knowledge base\n    results = vectorstore.similarity_search(query, k=3)\n    # get the text from the results\n    source_knowledge = \"\\n\".join([x.page_content for x in results])\n    # feed into an augmented prompt\n    augmented_prompt = f\"\"\"Using the contexts below, answer the query.\n\n    Contexts:\n    {source_knowledge}\n\n    Query: {query}\"\"\"\n    return augmented_prompt","metadata":{"execution":{"iopub.status.busy":"2024-01-11T16:49:39.969520Z","iopub.execute_input":"2024-01-11T16:49:39.970481Z","iopub.status.idle":"2024-01-11T16:49:39.976887Z","shell.execute_reply.started":"2024-01-11T16:49:39.970436Z","shell.execute_reply":"2024-01-11T16:49:39.975771Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Create our augmented prompt:","metadata":{}},{"cell_type":"code","source":"print(augment_prompt(query))","metadata":{"execution":{"iopub.status.busy":"2024-01-11T16:49:45.933223Z","iopub.execute_input":"2024-01-11T16:49:45.934248Z","iopub.status.idle":"2024-01-11T16:49:46.210450Z","shell.execute_reply.started":"2024-01-11T16:49:45.934212Z","shell.execute_reply":"2024-01-11T16:49:46.209549Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There is still a lot of text here, so let's pass it onto our chat model by converting to Human Message ","metadata":{}},{"cell_type":"code","source":"# create a new user prompt\nprompt = HumanMessage(\n    content=augment_prompt(query)\n)\n# add to messages\nmessages.append(prompt)\n\nres = chat(messages)\n\nprint(res.content)","metadata":{"execution":{"iopub.status.busy":"2024-01-11T16:50:01.073404Z","iopub.execute_input":"2024-01-11T16:50:01.074182Z","iopub.status.idle":"2024-01-11T16:50:02.442499Z","shell.execute_reply.started":"2024-01-11T16:50:01.074148Z","shell.execute_reply":"2024-01-11T16:50:02.441524Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## WITHOUT RAG ","metadata":{}},{"cell_type":"code","source":"prompt = HumanMessage(\n    content=\"what is leptonic decay?give study details about xmath\"\n)\n\nres = chat(messages + [prompt])\nprint(res.content)","metadata":{"execution":{"iopub.status.busy":"2024-01-11T08:07:47.951934Z","iopub.execute_input":"2024-01-11T08:07:47.952676Z","iopub.status.idle":"2024-01-11T08:07:54.580629Z","shell.execute_reply.started":"2024-01-11T08:07:47.952645Z","shell.execute_reply":"2024-01-11T08:07:54.579726Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The chatbot is able to respond about Leptonic decay  However, it doesn't know anything about the safety measures xmath parameters inside the scientific papers, lets use RAG chatbot for this purpose, ","metadata":{}},{"cell_type":"markdown","source":"## WITH RAG","metadata":{}},{"cell_type":"code","source":"prompt = HumanMessage(\n    content=augment_prompt(\n        \"what is leptonic decay? give study details about xmath\"\n    )\n)\n\nres = chat(messages + [prompt])\nprint(res.content)","metadata":{"execution":{"iopub.status.busy":"2024-01-11T08:08:41.078265Z","iopub.execute_input":"2024-01-11T08:08:41.078743Z","iopub.status.idle":"2024-01-11T08:08:50.546988Z","shell.execute_reply.started":"2024-01-11T08:08:41.078708Z","shell.execute_reply":"2024-01-11T08:08:50.546010Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This response clearly highlights the xmath parameters specific to scientific papers unlike the LLM response , hence RAG chatbot is working as expected by giving detail response regarding Leptonic Decay and the xmath parameters","metadata":{}}]}